{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Daw0QELi6gQ",
        "outputId": "f46f2c67-d9dc-4ab0-ee55-2f431b474a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "#Sprawdzenie dostępności GPU\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pobranie danych HIGGS (~2.5 GB)\n",
        "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\n",
        "!gunzip -f HIGGS.csv.gz"
      ],
      "metadata": {
        "id": "dpj_ch1bj9Wl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pipeline danych\n",
        "CSV_FILE = \"HIGGS.csv\"\n",
        "FEATURE_DIM = 28\n",
        "BATCH_SIZE = 1024\n"
      ],
      "metadata": {
        "id": "wzlFvzaukPJm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_csv(line):\n",
        "    defaults = [[0.0]] * (FEATURE_DIM + 1)\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
        "    label = fields[0]\n",
        "    features = tf.stack(fields[1:])\n",
        "    return features, label"
      ],
      "metadata": {
        "id": "2doUw6xGkRtw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = (\n",
        "    tf.data.TextLineDataset(CSV_FILE)\n",
        "    .map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .shuffle(100_000)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "5MxnLGoLkXmK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset.take(1000)  # ≈ 1 mln przykładów\n"
      ],
      "metadata": {
        "id": "px7uHNS7kjti"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model na GPU\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(FEATURE_DIM,)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "IjWhNFLtkngx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trening (na GPU)\n",
        "model.fit(train_ds, epochs=5)\n",
        "\n",
        "#Ewaluacja (opcjonalnie)\n",
        "eval_ds = dataset.skip(1000).take(200)\n",
        "model.evaluate(eval_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZtIXog4k6nl",
        "outputId": "d1904ab2-605b-440c-a9cc-09820ba0c6bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 84ms/step - accuracy: 0.6269 - loss: 0.6402\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 81ms/step - accuracy: 0.6861 - loss: 0.5881\n",
            "Epoch 3/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 80ms/step - accuracy: 0.6989 - loss: 0.5723\n",
            "Epoch 4/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 82ms/step - accuracy: 0.7085 - loss: 0.5606\n",
            "Epoch 5/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 83ms/step - accuracy: 0.7164 - loss: 0.5513\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 81ms/step - accuracy: 0.7228 - loss: 0.5414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5417237281799316, 0.7232568264007568]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}