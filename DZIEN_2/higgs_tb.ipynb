{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18nJavH2l9gM",
        "outputId": "8d240dc4-beb7-4eba-9285-636b6b5616c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Sprawdzenie GPU\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Pobranie danych\n",
        "import urllib.request\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"HIGGS.csv\"):\n",
        "    print(\"Pobieranie danych...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\", \"HIGGS.csv.gz\"\n",
        "    )\n",
        "    with gzip.open(\"HIGGS.csv.gz\", \"rb\") as f_in, open(\"HIGGS.csv\", \"wb\") as f_out:\n",
        "        f_out.write(f_in.read())\n",
        "\n"
      ],
      "metadata": {
        "id": "3J1Oab39mQra"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Pipeline danych\n",
        "CSV_FILE = \"HIGGS.csv\"\n",
        "FEATURE_DIM = 28\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "def parse_csv(line):\n",
        "    defaults = [[0.0]] * (FEATURE_DIM + 1)\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
        "    label = fields[0]\n",
        "    features = tf.stack(fields[1:])\n",
        "    return features, label\n",
        "\n",
        "# Apply batching before taking/skipping\n",
        "dataset = (\n",
        "    tf.data.TextLineDataset(CSV_FILE)\n",
        "    .map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .shuffle(100_000)\n",
        "    .batch(BATCH_SIZE) # Batch here\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Now take/skip a specific number of batches\n",
        "# Let's calculate the number of batches in the original dataset first\n",
        "# Note: This can be computationally expensive for large datasets without specifying batch size first\n",
        "# A more efficient way might be to use dataset.cardinality().numpy() if the cardinality is known\n",
        "# For now, let's just use the specified splits and check their sizes.\n",
        "\n",
        "train_batches_to_take = 1000\n",
        "val_batches_to_take = 200\n",
        "test_batches_to_take = 200\n",
        "val_batches_to_skip = 1000\n",
        "test_batches_to_skip = 1200\n",
        "\n",
        "train_ds = dataset.take(train_batches_to_take)           # take 1000 batches\n",
        "val_ds   = dataset.skip(val_batches_to_skip).take(val_batches_to_take) # skip 1000 batches, take 200\n",
        "test_ds  = dataset.skip(test_batches_to_skip).take(test_batches_to_take) # skip 1200 batches, take 200\n",
        "\n",
        "# Print the size of the datasets to verify they are not empty\n",
        "# Note: dataset.cardinality().numpy() can return -1 if the size is not known.\n",
        "# If it returns -1, you might need to iterate through the dataset to get the exact size,\n",
        "# which can be slow, or estimate based on file size and batch size.\n",
        "print(f\"Train dataset cardinality: {train_ds.cardinality().numpy()}\")\n",
        "print(f\"Validation dataset cardinality: {val_ds.cardinality().numpy()}\")\n",
        "print(f\"Test dataset cardinality: {test_ds.cardinality().numpy()}\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(FEATURE_DIM,)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Callbacki\n",
        "log_dir = \"logs/higgs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Trening\n",
        "# Only proceed with training if the datasets are not empty\n",
        "if train_ds.cardinality().numpy() > 0 and val_ds.cardinality().numpy() > 0:\n",
        "    print(\"Starting training...\")\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        epochs=20,\n",
        "        validation_data=val_ds,\n",
        "        callbacks=[tensorboard_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    # Ewaluacja\n",
        "    if test_ds.cardinality().numpy() > 0:\n",
        "        print(\"Starting evaluation...\")\n",
        "        loss, acc, auc = model.evaluate(test_ds)\n",
        "        print(f\"Test Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
        "    else:\n",
        "        print(\"Test dataset is empty, skipping evaluation.\")\n",
        "else:\n",
        "    print(\"Train or validation dataset is empty, skipping training and evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpPcLnZ_pcEy",
        "outputId": "48607afe-4b48-4527-fa45-2a3482b7675e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset cardinality: -2\n",
            "Validation dataset cardinality: -2\n",
            "Test dataset cardinality: -2\n",
            "Train or validation dataset is empty, skipping training and evaluation.\n"
          ]
        }
      ]
    }
  ]
}